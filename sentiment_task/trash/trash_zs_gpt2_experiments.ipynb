{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/diego/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import transformers\n",
    "import sklearn\n",
    "# import lxml\n",
    "nltk.download('punkt')\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.prompts.prefix_tuning_template import PrefixTuningTemplate\n",
    "from openprompt import PromptForGeneration, PromptDataLoader\n",
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.utils import shuffle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download models\n",
    "and store them in a local directory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d59e5281f20425dbae3476a4f22ab8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc250b9304754d44858d88277336e797"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bddac428ee994c5c8e54aa19580405ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "848786a634f5486ba7df08f8f403dc86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a11f4353c96c432a9d5de4fc3a0d75d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "('tokenizers/gpt2-medium/tokenizer_config.json',\n 'tokenizers/gpt2-medium/special_tokens_map.json',\n 'tokenizers/gpt2-medium/vocab.json',\n 'tokenizers/gpt2-medium/merges.txt',\n 'tokenizers/gpt2-medium/added_tokens.json')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load \"gpt2-medium\" with tokenizer\n",
    "tok = transformers.GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "lm = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id = tok.eos_token_id)\n",
    "transformers.PreTrainedModel.save_pretrained(lm, \"models/gpt2-medium\")\n",
    "transformers.PreTrainedTokenizer.save_pretrained(tok, \"tokenizers/gpt2-medium\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fb87fef9afd46218c901649aade1a1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eca311b0562e49108c671e55456f4ab5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fe83a47a23d469e83a079949208a6f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/666 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db2386c2ffce43b5af34cc3093e880cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/3.02G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b393bfa6adc468dad1b7a98d14b4df9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load \"gpt2-large\" with tokenizer\n",
    "tok = transformers.GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "lm = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id = tok.eos_token_id)\n",
    "transformers.PreTrainedModel.save_pretrained(lm, \"models/gpt2-large\")\n",
    "transformers.PreTrainedTokenizer.save_pretrained(tok, \"tokenizers/gpt2-large\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "('tokenizers/gpt2-medium/tokenizer_config.json',\n 'tokenizers/gpt2-medium/special_tokens_map.json',\n 'tokenizers/gpt2-medium/vocab.json',\n 'tokenizers/gpt2-medium/merges.txt',\n 'tokenizers/gpt2-medium/added_tokens.json')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load \"gpt2-xl\" with tokenizer\n",
    "# tok = transformers.GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "# lm = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id = tok.eos_token_id)\n",
    "# transformers.PreTrainedModel.save_pretrained(lm, \"models/gpt2-medium\")\n",
    "# transformers.PreTrainedTokenizer.save_pretrained(tok, \"tokenizers/gpt2-medium\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "('tokenizers/distilbert-sst2/tokenizer_config.json',\n 'tokenizers/distilbert-sst2/special_tokens_map.json',\n 'tokenizers/distilbert-sst2/vocab.txt',\n 'tokenizers/distilbert-sst2/added_tokens.json',\n 'tokenizers/distilbert-sst2/tokenizer.json')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "lm = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "transformers.PreTrainedModel.save_pretrained(lm, \"models/distilbert-sst2\")\n",
    "transformers.PreTrainedTokenizer.save_pretrained(tok, \"tokenizers/distilbert-sst2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments: methodology\n",
    "\n",
    "- input: list of models, list of templates, list of generation params\n",
    "- datasets: training and dev\n",
    "\n",
    "-for a specific template:\n",
    "--- for each model version:\n",
    "----- generate counterfactuals for the training set, for a specific params cfg;\n",
    "----- select the best params cfg based on the evaluation on the training set;\n",
    "----- generate the counterfactuals for the dev set;\n",
    "----- evaluate the generated counterfactuals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEMPLATE #1\n",
    "\n",
    "[label_example] review: [example text]\n",
    "\n",
    "[label_counterfactual] review: [generation token]\n",
    "\n",
    "Label template 0:\"Negative\"; 1:\"Positive\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "label_for_template = {0:\"Negative\", 1:\"Positive\"}\n",
    "template = '[label_a] review: [text_a]\\n[label_b] review:'\n",
    "\n",
    "# prompts need to provide the \"mask\" token at the end of the template\n",
    "# because GPT2 can only predict the next word based on the past context\n",
    "template_prompt = '{\"placeholder\":\"text_a\"}{\"mask\"}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEMPLATE #2\n",
    "\n",
    "The movie is [label_example]. [example text]\n",
    "\n",
    "The movie is [label_counterfactual]. [generation token]\n",
    "\n",
    "Label template 0:\"bad\"; 1:\"good\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "label_for_template = {0:\"bad\", 1:\"good\"}\n",
    "template = 'The movie is [label_a]. [text_a]\\nThe movie is [label_b].'\n",
    "\n",
    "# prompts need to provide the \"mask\" token at the end of the template\n",
    "# because GPT2 can only predict the next word based on the past context\n",
    "template_prompt = '{\"placeholder\":\"text_a\"}{\"mask\"}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of configurations for generation:\n",
      "[(4, 5, 0), (4, 5, 0.8), (8, 5, 0), (8, 5, 0.8), (12, 5, 0), (12, 5, 0.8)]\n",
      "# of hypers cfgs:6\n"
     ]
    }
   ],
   "source": [
    "random_seed = 5\n",
    "allow_parallelization = False\n",
    "empty_gpu_memory = True\n",
    "generator_on_cuda = True\n",
    "reduce_data = True\n",
    "data_to_keep = 400\n",
    "\n",
    "# model_names = [\"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "model_names = {\"gpt2-medium\"}\n",
    "gen_params_grid = {\"no_repeat_ngram_size\":[4, 8, 12],\n",
    "                   \"num_beam\":[5],\n",
    "                   \"temperature\":[0, 0.8]}\n",
    "sentiment_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# get all the entries of the grid\n",
    "all_pars = sorted(gen_params_grid)\n",
    "combinations_grid = list(itertools.product(*(gen_params_grid[par] for par in all_pars)))\n",
    "print(\"List of configurations for generation:\")\n",
    "print(combinations_grid)\n",
    "print(f\"# of hypers cfgs:{len(combinations_grid)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the best hyperparameters settings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 17:37:01.214853: Begin experiments for model gpt2-medium.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 17:37:18.362114:Model and tokenizer loaded.\n",
      "CFG tested: (4, 5, 0)\n",
      "Dataset's Dataframe prepared\n",
      "# of data points in the dataset:  400\n",
      "Dataset examples wrapped with prompt template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 400it [00:00, 473.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 17:37:22.048809: Begin of the generation.\n",
      "Total GPU memory available: 4240244736\n",
      "Allocated GPU memory before generation: 1444470784\n",
      "Allocated GPU memory reserved: 1447034880\n",
      "Allocated GPU memory before generation: 1578193408\n",
      "2022-01-28 17:37:25.236165, Step:0: 100 counterfactuals generated\n",
      "Allocated GPU memory before generation: 1652913664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_143788/3414210210.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     46\u001B[0m                            \u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m                            \u001B[0mprompt_for_generation\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                            cfg_gen)\n\u001B[0m\u001B[1;32m     49\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{datetime.datetime.now()}: End of the generation.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_143788/2497379146.py\u001B[0m in \u001B[0;36mperform_generation\u001B[0;34m(data_loader_, dataset, tokenizer_, generator, cfg_for_gen)\u001B[0m\n\u001B[1;32m     46\u001B[0m             _, generated_counter = generator.generate(inputs_to_device,\n\u001B[1;32m     47\u001B[0m                                                               \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                                                               **generation_arguments)\n\u001B[0m\u001B[1;32m     49\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minstance_guid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, batch, verbose, **generation_kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate_ith_token\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m                 \u001B[0moutput_sequence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minstance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minput_generation_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpad_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meos_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meos_token_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m                 \u001B[0moutput_sequences\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_sequence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# TODO: to support generate multiple sentence\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1078\u001B[0m                 \u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1079\u001B[0m                 \u001B[0msynced_gpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msynced_gpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1080\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1081\u001B[0m             )\n\u001B[1;32m   1082\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mbeam_search\u001B[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1824\u001B[0m             )  # (batch_size * num_beams, vocab_size)\n\u001B[1;32m   1825\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1826\u001B[0;31m             \u001B[0mnext_token_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlogits_processor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnext_token_scores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1827\u001B[0m             \u001B[0mnext_token_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext_token_scores\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mbeam_scores\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand_as\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnext_token_scores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1828\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_logits_process.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, input_ids, scores, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m                 \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocessor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 95\u001B[0;31m                 \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprocessor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     96\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_logits_process.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, input_ids, scores)\u001B[0m\n\u001B[1;32m    296\u001B[0m         \u001B[0mnum_batch_hypotheses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    297\u001B[0m         \u001B[0mcur_len\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 298\u001B[0;31m         \u001B[0mbanned_batch_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_calc_banned_ngram_tokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mngram_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_batch_hypotheses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcur_len\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    299\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbanned_tokens\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbanned_batch_tokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_logits_process.py\u001B[0m in \u001B[0;36m_calc_banned_ngram_tokens\u001B[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_hypos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 271\u001B[0;31m     \u001B[0mgenerated_ngrams\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_ngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mngram_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprev_input_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_hypos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    272\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    273\u001B[0m     banned_tokens = [\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_logits_process.py\u001B[0m in \u001B[0;36m_get_ngrams\u001B[0;34m(ngram_size, prev_input_ids, num_hypos)\u001B[0m\n\u001B[1;32m    246\u001B[0m     \u001B[0mgenerated_ngrams\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_hypos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_hypos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 248\u001B[0;31m         \u001B[0mgen_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprev_input_ids\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    249\u001B[0m         \u001B[0mgenerated_ngram\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerated_ngrams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mngram\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mgen_tokens\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mngram_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for model_name in model_names:\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"{datetime.datetime.now()}: Begin experiments for model {model_name}.\")\n",
    "    _, tokenizer, model_config, WrapperClass = load_plm(model_name=\"gpt2\", model_path=model_name)\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(\"tokenizers/\" + model_name)\n",
    "    plm = GPT2LMHeadModel.from_pretrained(\"models/\" + model_name, pad_token_id = tokenizer.eos_token_id)\n",
    "    print(f\"{datetime.datetime.now()}:Model and tokenizer loaded.\")\n",
    "\n",
    "    for cfg_gen in combinations_grid:\n",
    "\n",
    "        print(f\"CFG tested: {cfg_gen}\")\n",
    "        # load and prepare training set\n",
    "        training_set = load_dataset(\"train_paired.tsv\")\n",
    "        training_set = prepare_dataset(training_set,\n",
    "                                       tokenizer,\n",
    "                                       reduce_data,\n",
    "                                       data_to_keep,\n",
    "                                       random_seed)\n",
    "\n",
    "        prompt_template = ManualTemplate(\n",
    "            text = template_prompt,\n",
    "            tokenizer = tokenizer,\n",
    "        )\n",
    "\n",
    "        training_set.wrap_dataset_instances(template_=template,\n",
    "                                            label_template=label_for_template)\n",
    "\n",
    "        data_loader = PromptDataLoader(\n",
    "            dataset = list(training_set.get_dataset().values()),\n",
    "            tokenizer = tokenizer,\n",
    "            template = prompt_template,\n",
    "            tokenizer_wrapper_class=WrapperClass,\n",
    "        )\n",
    "\n",
    "        # set the template and the generator\n",
    "        prompt_for_generation = set_generator(prompt_template,\n",
    "                                              plm,\n",
    "                                              allow_parallelization,\n",
    "                                              generator_on_cuda)\n",
    "\n",
    "        print(f\"{datetime.datetime.now()}: Begin of the generation.\")\n",
    "        # generate counterfactuals\n",
    "        perform_generation(data_loader,\n",
    "                           training_set,\n",
    "                           tokenizer,\n",
    "                           prompt_for_generation,\n",
    "                           cfg_gen)\n",
    "        print(f\"{datetime.datetime.now()}: End of the generation.\")\n",
    "\n",
    "        # create the Evaluator object\n",
    "        print(f\"{datetime.datetime.now()}: Begin of evaluation.\")\n",
    "        sent_tokenizer, sent_model, sent_dict = load_sentiment_model(sentiment_model_name)\n",
    "        sent_device = 0 if torch.cuda.is_available() else -1\n",
    "        # print(sent_device)\n",
    "        evaluator = Evaluator(training_set,\n",
    "                              sent_tokenizer,\n",
    "                              sent_model,\n",
    "                              sent_dict,\n",
    "                              sent_device)\n",
    "\n",
    "        # predict labels for counterfactuals\n",
    "        pred_labels, avg_confidence = evaluator.infer_predictions()\n",
    "        # pred_labels = [1, 1]\n",
    "\n",
    "        # calculate the LFS\n",
    "        true_labels = [training_set.get_instance_by_id(guid).meta[\"label_b\"] for guid in training_set]\n",
    "        lfs = evaluator.lf_score(true_labels, pred_labels)\n",
    "\n",
    "        # calculate BLUE\n",
    "        mean_blue, var_blue = evaluator.blue_score()\n",
    "        print(f\"{datetime.datetime.now()}: End of the evaluation.\")\n",
    "        print(f\"{model_name}| LFS:{lfs}, AVG_CONF:{avg_confidence}, mean_BLUE:{mean_blue}, var_BLUE:{var_blue}\")\n",
    "        results.append(f\"{model_name}| LFS:{lfs}, AVG_CONF:{avg_confidence}, mean_BLUE:{mean_blue}, var_BLUE:{var_blue}\")\n",
    "\n",
    "        print()\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"End of experiments for model {model_name}.\")\n",
    "    print(f\"Total execution time for model {model_name}: {toc - tic:0.4f}.\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['gpt2-medium| LFS:0.5725, AVG_CONF:0.9690174861252308, mean_BLUE:0.10141909904553367, var_BLUE:0.021700017855491926',\n 'gpt2-medium| LFS:0.565, AVG_CONF:0.9737502524256706, mean_BLUE:0.17558427443312283, var_BLUE:0.04844216497166922',\n 'gpt2-medium| LFS:0.53, AVG_CONF:0.9729126597940921, mean_BLUE:0.1712076253204455, var_BLUE:0.04703537604798624']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"https://www.allrecipes.com/recipes/96/salad/\")\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Classes to be used</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def reformat_sentiment(x):\n",
    "    return int(x == 'Positive')\n",
    "\n",
    "def load_dataset(name):\n",
    "    # load the dataset\n",
    "    url = 'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/' + name\n",
    "    dataset = pd.read_csv(url, sep='\\t')\n",
    "    dataset.rename(columns={\"Sentiment\": \"sentiment\", \"Text\": \"text\", \"batch_id\": \"paired_id\"}, inplace=True)\n",
    "    # reformat 'sentiment' column\n",
    "    dataset['sentiment'] = dataset['sentiment'].apply(lambda value: reformat_sentiment(value))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def prepare_dataset(dataframe_, tokenizer_, reduce_dataset_, n_to_keep_, seed=1):\n",
    "    # prepare the dataset for the generation\n",
    "    dataset = SentimentDataset(loaded_dataset=dataframe_,\n",
    "                               tokenizer_=tokenizer_,\n",
    "                               max_length=1024)\n",
    "    dataset.randomly_assign_conterfactuals(seed)\n",
    "    dataset.prepare_dataframe_with_counterfacuals(reduce_dataset_,\n",
    "                                                  n_to_keep_,\n",
    "                                                  seed)\n",
    "    dataset.prepare_dataset()\n",
    "    print(\"# of data points in the dataset: \", len(dataset))\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, loaded_dataset, tokenizer_, max_length):\n",
    "        # get a copy of the dataset\n",
    "        self.dataframe = loaded_dataset.copy()\n",
    "        self.tokenizer = tokenizer_\n",
    "        self.max_length = max_length\n",
    "        self.dataframe_with_counterfactuals = None\n",
    "        self.guids = []\n",
    "        self.dataset = {}\n",
    "        self.dataset_with_prompts = []\n",
    "\n",
    "    def randomly_assign_conterfactuals(self, seed=1):\n",
    "        # prepare the proper Dataframe for the dataset\n",
    "        self.random_shuffle(seed)\n",
    "        paired_ids = self.dataframe['paired_id'].values\n",
    "        found_ids = {}\n",
    "        counterfactual_column = []\n",
    "        for id in paired_ids:\n",
    "            counterfactual_column.append(self.__set_example_counter__(id, found_ids))\n",
    "        self.dataframe['is_counterfactual'] = counterfactual_column\n",
    "\n",
    "        print(\"Dataset's Dataframe prepared\")\n",
    "\n",
    "    def __set_example_counter__(self, idx, found_idsx):\n",
    "        if idx in found_idsx:\n",
    "            return 0\n",
    "        else:\n",
    "            found_idsx[idx] = 0\n",
    "            return 1\n",
    "\n",
    "        # prepare a dataset with input-output instances\n",
    "    def prepare_dataframe_with_counterfacuals(self,\n",
    "                                              reduce_dataset_,\n",
    "                                              n_to_keep_,\n",
    "                                              seed):\n",
    "\n",
    "        # group by paired_id\n",
    "        gb = self.dataframe.groupby(by=[\"paired_id\"])\n",
    "\n",
    "        # create new columns \"example\" and \"counterfactual\"\n",
    "        example_column = []\n",
    "        counter_column = []\n",
    "        paired_id_column = []\n",
    "        label_ex = []\n",
    "        label_counter = []\n",
    "        for group_id in gb.groups: # group_id == paired_id\n",
    "            group = gb.get_group(group_id)\n",
    "            is_counterfactual_column = group['is_counterfactual'].values\n",
    "            text_column = group['text'].values\n",
    "            sentiment_column = group['sentiment'].values\n",
    "            for is_counter, text, label in zip(is_counterfactual_column,\n",
    "                                               text_column,\n",
    "                                               sentiment_column):\n",
    "                if is_counter:\n",
    "                    counter_column.append(text)\n",
    "                    label_counter.append(label)\n",
    "                else:\n",
    "                    example_column.append(text)\n",
    "                    label_ex.append(label)\n",
    "\n",
    "            paired_id_column.append(group_id)\n",
    "\n",
    "        # add the new columns to a new dataframe\n",
    "        d = {'paired_id': paired_id_column,\n",
    "             'example': example_column,\n",
    "             'label_ex': label_ex,\n",
    "             'counterfactual': counter_column,\n",
    "             'label_counter': label_counter}\n",
    "        self.dataframe_with_counterfactuals = pd.DataFrame(data=d)\n",
    "\n",
    "        if reduce_dataset_:\n",
    "            self.dataframe_with_counterfactuals = self.dataframe_with_counterfactuals.sample(n=n_to_keep_, random_state=seed)\n",
    "            self.dataframe_with_counterfactuals.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # convert the Dataframe into the InputExample format dataset of openprompt\n",
    "    def prepare_dataset(self):\n",
    "        for index, row in self.dataframe_with_counterfactuals.iterrows():\n",
    "            self.dataset[row['paired_id']] = InputExample(guid=row['paired_id'],\n",
    "                                                          text_a=BeautifulSoup(\n",
    "                                                              row['example'], \"lxml\").text,\n",
    "                                                          text_b=BeautifulSoup(\n",
    "                                                              row['counterfactual'], \"lxml\").text,\n",
    "                                                          meta={\"label_a\":row['label_ex'],\n",
    "                                                                \"label_b\":row['label_counter'],\n",
    "                                                                'example':BeautifulSoup(\n",
    "                                                                    row['example'], \"lxml\").text,\n",
    "                                                                'counterfactual':BeautifulSoup(\n",
    "                                                                    row['counterfactual'], \"lxml\").text})\n",
    "            self.guids.append(row['paired_id'])\n",
    "\n",
    "    def wrap_dataset_instances(self, template_, label_template):\n",
    "        # template is a string with the whole template\n",
    "        # label_template is a dict with the mapping between label and template\n",
    "        for idx in self.dataset:\n",
    "            instance = self.dataset[idx]\n",
    "            instance.text_a = template_.replace('[text_a]', instance.text_a)\n",
    "            instance.text_a = instance.text_a.replace(\n",
    "                '[label_a]', label_template[instance.meta['label_a']])\n",
    "            instance.text_a = instance.text_a.replace(\n",
    "                '[label_a]', label_template[instance.meta['label_a']])\n",
    "            instance.text_a = instance.text_a.replace(\n",
    "                '[label_b]', label_template[instance.meta['label_b']])\n",
    "\n",
    "        print('Dataset examples wrapped with prompt template')\n",
    "\n",
    "    def sort_by_paired_id(self):\n",
    "        self.dataframe.sort_values('paired_id', inplace=True)\n",
    "\n",
    "    def sort_prompted_by_paired_id(self):\n",
    "        self.dataframe_with_prompts.sort_values('paired_id', inplace=True)\n",
    "\n",
    "    def random_shuffle(self, seed):\n",
    "        random.seed(seed)\n",
    "        self.dataframe = shuffle(self.dataframe)\n",
    "\n",
    "    # the same of __getitem__\n",
    "    def get_instance_by_id(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    # implemented because of inheritance from Dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # implemented because of inheritance from Dataset\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    def __next__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    # implemented because of inheritance from Dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.__getitem__(idx)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "\n",
    "    def get_dataframe_with_counterfactuals(self):\n",
    "        return self.dataframe_with_counterfactuals\n",
    "\n",
    "    def get_dataset_with_prompts(self):\n",
    "        return self.dataset_with_prompts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def set_generator(prompt_template_, plm_, parallelization, cuda_gen):\n",
    "\n",
    "    prompt_for_generation_ = PromptForGeneration(\n",
    "        template = prompt_template_,\n",
    "        freeze_plm = True,\n",
    "        plm = plm_,\n",
    "        plm_eval_mode = True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available() and cuda_gen:\n",
    "        prompt_for_generation_ = prompt_for_generation_.cuda()\n",
    "\n",
    "    if parallelization:\n",
    "        prompt_for_generation_.parallelize()\n",
    "\n",
    "    return prompt_for_generation_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def perform_generation(data_loader_,\n",
    "                       dataset,\n",
    "                       tokenizer_,\n",
    "                       generator,\n",
    "                       cfg_for_gen):\n",
    "    generator.eval()\n",
    "\n",
    "    print(f\"Total GPU memory available: {torch.cuda.get_device_properties(0).total_memory}\")\n",
    "    print(f\"Allocated GPU memory before generation: {torch.cuda.memory_allocated(0)}\")\n",
    "    print(f\"Allocated GPU memory reserved: {torch.cuda.memory_reserved(0)}\")\n",
    "    for (step, inputs) in enumerate(data_loader_):\n",
    "\n",
    "        # retrieve the instance involved\n",
    "        instance_guid = inputs[\"guid\"].numpy()[0]\n",
    "        instance_to_update = dataset.get_instance_by_id(instance_guid)\n",
    "\n",
    "        # we limit the output length to be reasonably equal to the input\n",
    "        # context, i.e. the example\n",
    "        max_length_example = len(tokenizer_.encode(instance_to_update.text_a))\n",
    "        max_length_output = int(2 * max_length_example)\n",
    "\n",
    "        # cfg_gen[0] = no_repeat_ngram_size\n",
    "        # cfg_gen[1] = num_beam\n",
    "        # cfg_gen[2] = temperature\n",
    "        generation_arguments = {\n",
    "            \"max_length\": max_length_output,\n",
    "            \"min_length\": 5,\n",
    "            \"no_repeat_ngram_size\": cfg_for_gen[0],\n",
    "            \"num_beams\": cfg_for_gen[1],\n",
    "            \"temperature\": cfg_for_gen[2],\n",
    "            \"do_sample\": False,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0,\n",
    "            # \"repetition_penalty\": 2.0,\n",
    "            # \"num_return_sequences\": 3\n",
    "            # \"early_stopping\": True\n",
    "        }\n",
    "        # print(generation_arguments)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_to_device = inputs.cuda()\n",
    "        else:\n",
    "            inputs_to_device = inputs\n",
    "\n",
    "        try:\n",
    "            _, generated_counter = generator.generate(inputs_to_device,\n",
    "                                                              verbose=True,\n",
    "                                                              **generation_arguments)\n",
    "        except Exception as e:\n",
    "            print(instance_guid)\n",
    "            print(e)\n",
    "\n",
    "        # insert the generated counterfactual\n",
    "        instance_to_update.meta[\"generated_counter\"] = generated_counter[0]\n",
    "        # print(inputs[\"guid\"].numpy()[0])\n",
    "        if torch.cuda.is_available() and empty_gpu_memory:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Allocated GPU memory before generation: {torch.cuda.memory_allocated(0)}\")\n",
    "        if (step % 100) == 0:\n",
    "            print(f\"{datetime.datetime.now()}, Step:{step}: 100 counterfactuals generated\")\n",
    "            # if torch.cuda.is_available():\n",
    "            #   torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_66854/122237637.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m     codeout, geunter = prompt_for_generation.generate(inputs,\n\u001B[1;32m     20\u001B[0m                                    \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m                                    **gen_argumvsvs)\n\u001B[0m\u001B[1;32m     22\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgeunter\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, batch, verbose, **generation_kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate_ith_token\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m                 \u001B[0moutput_sequence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minstance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minput_generation_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpad_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meos_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meos_token_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m                 \u001B[0moutput_sequences\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_sequence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# TODO: to support generate multiple sentence\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1013\u001B[0m                 \u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1014\u001B[0m                 \u001B[0msynced_gpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msynced_gpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1015\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1016\u001B[0m             )\n\u001B[1;32m   1017\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgreedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1310\u001B[0m                 \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1311\u001B[0m                 \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1312\u001B[0;31m                 \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1313\u001B[0m             )\n\u001B[1;32m   1314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m         \"\"\"\n\u001B[1;32m    416\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1058\u001B[0m         )\n\u001B[1;32m   1059\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    828\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    829\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 830\u001B[0;31m             \u001B[0minputs_embeds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwte\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    831\u001B[0m         \u001B[0mposition_embeds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwpe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mposition_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    832\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mposition_embeds\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    158\u001B[0m         return F.embedding(\n\u001B[1;32m    159\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_norm\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 160\u001B[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001B[0m\u001B[1;32m    161\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2042\u001B[0m         \u001B[0;31m# remove once script supports set_grad_enabled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2043\u001B[0m         \u001B[0m_no_grad_embedding_renorm_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2044\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_grad_by_freq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msparse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2045\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2046\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "# inputsss = torch.tensor([1, 2, 3])\n",
    "# inputsss.cpu()\n",
    "# inputsss\n",
    "gen_argumvsvs = {\n",
    "    \"max_length\": 215,\n",
    "    \"min_length\": 5,\n",
    "    \"no_repeat_ngram_size\": 10,\n",
    "    \"num_beams\": 1,\n",
    "    \"temperature\": 0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 10,\n",
    "    \"top_p\": 0,\n",
    "    # \"repetition_penalty\": 2.0,\n",
    "    # \"num_return_sequences\": 3\n",
    "    # \"early_stopping\": True\n",
    "}\n",
    "\n",
    "for (step, inputs) in enumerate(data_loader):\n",
    "    codeout, geunter = prompt_for_generation.generate(inputs,\n",
    "                                   verbose=True,\n",
    "                                   **gen_argumvsvs)\n",
    "\n",
    "    print(geunter[0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_for_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_123232/430619694.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprompt_for_generation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'prompt_for_generation' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_for_generation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_123232/730665603.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m     codeout, geunter = prompt_for_generation.generate(inputs.cuda(),\n\u001B[1;32m     19\u001B[0m                                                       \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m                                                       **gen_argumvsvs)\n\u001B[0m\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0;32mdel\u001B[0m \u001B[0mcodeout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, batch, verbose, **generation_kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate_ith_token\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m                 \u001B[0moutput_sequence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minstance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minput_generation_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpad_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meos_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meos_token_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m                 \u001B[0moutput_sequences\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_sequence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# TODO: to support generate multiple sentence\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1013\u001B[0m                 \u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1014\u001B[0m                 \u001B[0msynced_gpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msynced_gpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1015\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1016\u001B[0m             )\n\u001B[1;32m   1017\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgreedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1310\u001B[0m                 \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1311\u001B[0m                 \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1312\u001B[0;31m                 \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1313\u001B[0m             )\n\u001B[1;32m   1314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m         \"\"\"\n\u001B[1;32m    416\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1058\u001B[0m         )\n\u001B[1;32m   1059\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    828\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    829\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 830\u001B[0;31m             \u001B[0minputs_embeds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwte\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    831\u001B[0m         \u001B[0mposition_embeds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwpe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mposition_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    832\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mposition_embeds\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    158\u001B[0m         return F.embedding(\n\u001B[1;32m    159\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_norm\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 160\u001B[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001B[0m\u001B[1;32m    161\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2042\u001B[0m         \u001B[0;31m# remove once script supports set_grad_enabled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2043\u001B[0m         \u001B[0m_no_grad_embedding_renorm_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2044\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_grad_by_freq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msparse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2045\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2046\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "gen_argumvsvs = {\n",
    "    \"max_length\": 215,\n",
    "    \"min_length\": 5,\n",
    "    \"no_repeat_ngram_size\": 10,\n",
    "    \"num_beams\": 1,\n",
    "    \"temperature\": 0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 10,\n",
    "    \"top_p\": 0,\n",
    "    # \"repetition_penalty\": 2.0,\n",
    "    # \"num_return_sequences\": 3\n",
    "    # \"early_stopping\": True\n",
    "}\n",
    "\n",
    "# prompt_ation = prompt_for_generation\n",
    "\n",
    "for (step, inputs) in enumerate(data_loader):\n",
    "    _, geunter = prompt_for_generation.generate(inputs.cuda(),\n",
    "                                                      verbose=True,\n",
    "                                                      **gen_argumvsvs)\n",
    "\n",
    "    del codeout\n",
    "    print(geunter[0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, s_dataset, s_tokenizer, s_model, s_label_dict, s_device):\n",
    "        \"\"\"Constructor of the evaluator\n",
    "\n",
    "        @param: _sentiment_tokenizer The tokenizer for the sentiment classifier\n",
    "        @param: _sentiment_model The sentiment classifier\n",
    "        @param: _label_dict The mapping between classifier's outputs and labels\n",
    "        \"\"\"\n",
    "        self._dataset = s_dataset\n",
    "        self._sentiment_tokenizer = s_tokenizer\n",
    "        self._sentiment_model = s_model\n",
    "        self._label_dict = s_label_dict\n",
    "        self.classifier = transformers.pipeline(\n",
    "            task=\"sentiment-analysis\",\n",
    "            model=s_model,\n",
    "            tokenizer=s_tokenizer,\n",
    "            framework=\"pt\",\n",
    "            device=s_device)\n",
    "\n",
    "    def infer_predictions(self):\n",
    "\n",
    "        try:\n",
    "            texts = [self._dataset.get_instance_by_id(guid).meta[\"generated_counter\"] for guid in self._dataset]\n",
    "            # for guid in self._dataset:\n",
    "            #   self._dataset.get_instance_by_id(guid).meta[\"generated_counter\"]\n",
    "        except:\n",
    "            print(f\"There is an instance that does not have a counterfactual\")\n",
    "        predicted_labels = []\n",
    "        score_labels = []\n",
    "\n",
    "        for text_to_classify in texts:\n",
    "            if len(text_to_classify) > 512:\n",
    "                result = self.classifier(text_to_classify[:511])[0]\n",
    "            else:\n",
    "                result = self.classifier(text_to_classify)[0]\n",
    "\n",
    "            predicted_labels.append(self._label_dict[result['label']])\n",
    "            score_labels.append(result['score'])\n",
    "\n",
    "        return predicted_labels, np.mean(score_labels)\n",
    "\n",
    "\n",
    "    def lf_score(self, y_desired, y_pred):\n",
    "        \"\"\"Calculate the Label Flip Score (LFS)\n",
    "        \"\"\"\n",
    "        return sklearn.metrics.accuracy_score(y_desired, y_pred)\n",
    "\n",
    "    def blue_score(self):\n",
    "        \"\"\"Calculate the BLUE score for a pair of example-counter.\n",
    "\n",
    "           Returns mean and variance of the BLUE scores.\n",
    "        \"\"\"\n",
    "\n",
    "        BLEUscore = []\n",
    "        true_counters = [self._dataset.get_instance_by_id(guid).meta[\"counterfactual\"] for guid in self._dataset]\n",
    "        gen_counters = [self._dataset.get_instance_by_id(guid).meta[\"generated_counter\"] for guid in self._dataset]\n",
    "        for true_counter, gen_counter in zip(true_counters, gen_counters):\n",
    "            # example and counterfactual need to be tokenized first\n",
    "\n",
    "            # the reference is the true counterfactual\n",
    "            reference = nltk.tokenize.word_tokenize(true_counter)\n",
    "\n",
    "            # the hypothesis is the generated counterfactual\n",
    "            hypothesis = nltk.tokenize.word_tokenize(gen_counter)\n",
    "\n",
    "            BLEUscore.append(nltk.translate.bleu_score.sentence_bleu([reference],\n",
    "                                                                     hypothesis))\n",
    "        return np.mean(BLEUscore), np.var(BLEUscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": "def load_sentiment_model(name):\n    tokenizer_ = transformers.AutoTokenizer.from_pretrained(\"tokenizers/distilbert-sst2\")\n    model = transformers.AutoModelForSequenceClassification.from_pretrained(\"models/distilbert-sst2\")\n\n    if name == \"gchhablani/fnet-base-finetuned-sst2\":\n        return tokenizer_, model, {'negative':0, 'positive':1}\n\n    if name == \"siebert/sentiment-roberta-large-english\" or name == \"distilbert-base-uncased-finetuned-sst-2-english\":\n        return tokenizer_, model, {'NEGATIVE':0, 'POSITIVE':1}\n\n    return tokenizer_, model, {'LABEL_0':0, 'LABEL_1':1}",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": "",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "trusted": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}