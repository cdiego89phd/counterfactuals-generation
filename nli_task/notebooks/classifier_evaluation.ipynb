{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of different classifiers for NLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook evaluates which classifier is the best performing for the task of NLI classification. The best-performing classifier will be then used for evaluating the quality of the generated counterfactuals in the NLI task. This notebook is used only for illustration and debug purposes and results are not the definite one. Please run the script \"compare_nli_classifiers.py\" to perform the complete evaluation.\n",
    "\n",
    "Here a list of the classifiers that will be tested:\n",
    "- Roberta Large (the winner, with 77% of accuracy)\n",
    "- Distil Roberta\n",
    "- Bart Large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation procedure:\n",
    "- We take the Flickr Counterfactually-Augmented Dataset from Kaushik (cad_flickr_nli.tsv);\n",
    "- We merge the training and the val set to create an evaluation set\n",
    "- We use such evaluation set to test the performance of the various classifiers;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "to_debug = True\n",
    "N_TO_DEBUG = 12\n",
    "n_batches = 12\n",
    "\n",
    "eval_metrics = {\"precision\": datasets.load_metric(\"precision\"),\n",
    "                \"recall\": datasets.load_metric(\"recall\"),\n",
    "                \"f1\": datasets.load_metric(\"f1\"),\n",
    "                \"accuracy\": datasets.load_metric(\"accuracy\")\n",
    "                }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                        counter_prem  \\\n0  A man and three women are preparing a meal of ...   \n\n                       original_hyp counter_label task counter_hyp  \\\n0  A group of people cooking inside       neutral   RP         NaN   \n\n                                       original_prem original_label  \n0  A man and three women are preparing a meal ind...     entailment  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counter_prem</th>\n      <th>original_hyp</th>\n      <th>counter_label</th>\n      <th>task</th>\n      <th>counter_hyp</th>\n      <th>original_prem</th>\n      <th>original_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A man and three women are preparing a meal of ...</td>\n      <td>A group of people cooking inside</td>\n      <td>neutral</td>\n      <td>RP</td>\n      <td>NaN</td>\n      <td>A man and three women are preparing a meal ind...</td>\n      <td>entailment</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = pd.read_csv(\"../cad_flickr_nli/fold_0/training_set.tsv\", sep='\\t')\n",
    "valset = pd.read_csv(\"../cad_flickr_nli/fold_0/val_set.tsv\", sep='\\t')\n",
    "eval_data = pd.concat([trainset, valset], ignore_index=True)\n",
    "\n",
    "if to_debug:\n",
    "    eval_data = eval_data[:N_TO_DEBUG]\n",
    "eval_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(len(eval_data))\n",
    "eval_data.head(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract_prems(row):\n",
    "    if row[\"task\"] == \"RP\":\n",
    "        return row[\"counter_prem\"]\n",
    "    else:\n",
    "        return row[\"original_prem\"]\n",
    "\n",
    "def extract_hyps(row):\n",
    "    if row[\"task\"] == \"RH\":\n",
    "        return row[\"counter_hyp\"]\n",
    "    else:\n",
    "        return row[\"original_hyp\"]\n",
    "\n",
    "def generate_batches(bac, n):\n",
    "    batch_size = len(bac)//n\n",
    "    for i in range(0, len(bac), batch_size):\n",
    "        yield bac[i:i + batch_size]\n",
    "    return bac\n",
    "\n",
    "def evaluate_classifier(preds, labels, eval_m):\n",
    "    # evaluates a classifier\n",
    "    metrics = {\"precision\": eval_m[\"precision\"].compute(predictions=preds, references=labels, average=\"micro\")[\"precision\"],\n",
    "               \"recall\": eval_m[\"recall\"].compute(predictions=preds, references=labels, average=\"micro\")[\"recall\"],\n",
    "               \"f1\": eval_m[\"f1\"].compute(predictions=preds, references=labels, average=\"micro\")[\"f1\"],\n",
    "               \"accuracy\": eval_m[\"accuracy\"].compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "               }\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['The baby in the pink romper is crying.', 'The baby is happy.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data[\"premise\"] = eval_data.apply(lambda row: extract_prems(row), axis=1)\n",
    "eval_data[\"hypothesis\"] = eval_data.apply(lambda row: extract_hyps(row), axis=1)\n",
    "\n",
    "eval_batch = [[p, h] for p,h in zip(eval_data[\"premise\"].values, eval_data[\"hypothesis\"].values)]\n",
    "eval_batch[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Roberta large MNLI fine-tuned on MultiNLI\n",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/roberta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"neutral\": 1,\n",
    "             \"entailment\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "data = collate_tokens(\n",
    "    [model.encode(pair[0], pair[1]) for pair in eval_batch], pad_idx=1\n",
    ")\n",
    "batches = generate_batches(data, n_batches)\n",
    "predictions = []\n",
    "for batch in batches:\n",
    "    predictions += model.predict('mnli', batch).argmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DistilRoberta-base fine-tuned on SNLI and MultiNLI\n",
    "cross-encoder/nli-distilroberta-base\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"entailment\": 1,\n",
    "             \"neutral\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-distilroberta-base')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('cross-encoder/nli-distilroberta-base')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bart-large fine-tuned on MultiNLI\n",
    "facebook/bart-large-mnli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"neutral\": 1,\n",
    "             \"entailment\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeBerta base fine-tuned on SuperGLUE NLI\n",
    "microsoft/deberta-v3-base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "920300605db54aebad8a9b9aaec9d5e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e83a375473141ea8d24e8e60ef9ca6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e97c9d5688b943e6abea05e6377b0d73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22c21b7b20df48b5b06bb9cd6302d0d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"entailment\": 1,\n",
    "             \"neutral\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'precision': 0.6666666666666666,\n 'recall': 0.6666666666666666,\n 'f1': 0.6666666666666666,\n 'accuracy': 0.6666666666666666}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeBerta large fine-tuned on SuperGLUE NLI\n",
    "microsoft/deberta-v3-large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/580 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3ce19f9c00340069e567ce4ffd48393"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "757914b0f0574fceb6ec543d71de2633"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e7da9bd3c734f5e97c5b7a67809fb13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f916c41b6204475865b7b3438ac752b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"entailment\": 1,\n",
    "             \"neutral\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-large')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'precision': 0.6666666666666666,\n 'recall': 0.6666666666666666,\n 'f1': 0.6666666666666666,\n 'accuracy': 0.6666666666666666}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeBerta large fine-tuned on MNLI\n",
    "microsoft/deberta-large-mnli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/729 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fc916cf2fa648eeb7187d024507d077"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7bdc1de4036466889cf7a79d765f94d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46d015191d9b430496d04eb2ee18ce77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41d7cde15a00444e9c0fa1442d38c36d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1487af51b6f649349c5fb5be21dc093f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"entailment\": 1,\n",
    "             \"neutral\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-large-mnli')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('microsoft/deberta-large-mnli')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'precision': 0.6666666666666666,\n 'recall': 0.6666666666666666,\n 'f1': 0.6666666666666666,\n 'accuracy': 0.6666666666666666}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "cross-encoder/nli-deberta-v3-base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.03k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fb082494455470fae70b2e431d2b501"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/704M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81af122dd0904330a7bfe7e0e8863421"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/417 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44b5707bbfa44cfa99d6444226caa088"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "390550b4955249018a6922eb03b64dfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/18.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efa781aefe3f4071b363b4c0a9c816d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/156 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edd66251b21645598f90d6df81d5db77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"entailment\": 1,\n",
    "             \"neutral\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-base')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-base')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'precision': 0.5833333333333334,\n 'recall': 0.5833333333333334,\n 'f1': 0.5833333333333334,\n 'accuracy': 0.5833333333333334}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
