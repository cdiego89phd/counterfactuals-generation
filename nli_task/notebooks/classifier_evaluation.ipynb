{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of different classifiers for NLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook evaluates which classifier is the best performing for the task of NLI classification. The best-performing classifier will be then used for evaluating the quality of the generated counterfactuals in the NLI task. This notebook is used only for illustration and debug purposes and results are not the definite one. Please run the script \"compare_nli_classifiers.py\" to perform the complete evaluation.\n",
    "\n",
    "Here a list of the classifiers that will be tested:\n",
    "- Roberta Large\n",
    "- Distil Roberta\n",
    "- Bart Large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation procedure:\n",
    "- We take the Flickr Counterfactually-Augmented Dataset from Kaushik (cad_flickr_nli.tsv);\n",
    "- We merge the training and the val set to create an evaluation set\n",
    "- We use such evaluation set to test the performance of the various classifiers;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "to_debug = True\n",
    "N_TO_DEBUG = 10\n",
    "\n",
    "eval_metrics = {\"precision\": datasets.load_metric(\"precision\"),\n",
    "                \"recall\": datasets.load_metric(\"recall\"),\n",
    "                \"f1\": datasets.load_metric(\"f1\"),\n",
    "                \"accuracy\": datasets.load_metric(\"accuracy\")\n",
    "                }\n",
    "results = [] # to keep track of the results of different classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                        counter_prem  \\\n0  A man and three women are preparing a meal of ...   \n\n                       original_hyp counter_label task counter_hyp  \\\n0  A group of people cooking inside       neutral   RP         NaN   \n\n                                       original_prem original_label  \n0  A man and three women are preparing a meal ind...     entailment  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counter_prem</th>\n      <th>original_hyp</th>\n      <th>counter_label</th>\n      <th>task</th>\n      <th>counter_hyp</th>\n      <th>original_prem</th>\n      <th>original_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A man and three women are preparing a meal of ...</td>\n      <td>A group of people cooking inside</td>\n      <td>neutral</td>\n      <td>RP</td>\n      <td>NaN</td>\n      <td>A man and three women are preparing a meal ind...</td>\n      <td>entailment</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = pd.read_csv(\"../cad_flickr_nli/fold_0/training_set.tsv\", sep='\\t')\n",
    "valset = pd.read_csv(\"../cad_flickr_nli/fold_0/val_set.tsv\", sep='\\t')\n",
    "eval_data = pd.concat([trainset, valset], ignore_index=True)\n",
    "\n",
    "if to_debug:\n",
    "    eval_data = eval_data[:N_TO_DEBUG]\n",
    "eval_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(len(eval_data))\n",
    "eval_data.head(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def extract_prems(row):\n",
    "    if row[\"task\"] == \"RP\":\n",
    "        return row[\"counter_prem\"]\n",
    "    else:\n",
    "        return row[\"original_prem\"]\n",
    "\n",
    "def extract_hyps(row):\n",
    "    if row[\"task\"] == \"RH\":\n",
    "        return row[\"counter_hyp\"]\n",
    "    else:\n",
    "        return row[\"original_hyp\"]\n",
    "\n",
    "def evaluate_classifier(preds, labels, eval_m):\n",
    "    # evaluates a classifier\n",
    "    metrics = {\"precision\": eval_m[\"precision\"].compute(predictions=preds, references=labels, average=\"micro\")[\"precision\"],\n",
    "               \"recall\": eval_m[\"recall\"].compute(predictions=preds, references=labels, average=\"micro\")[\"recall\"],\n",
    "               \"f1\": eval_m[\"f1\"].compute(predictions=preds, references=labels, average=\"micro\")[\"f1\"],\n",
    "               \"accuracy\": eval_m[\"accuracy\"].compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "               }\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['The baby in the pink romper is crying.', 'The baby is happy.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data[\"premise\"] = eval_data.apply(lambda row: extract_prems(row), axis=1)\n",
    "eval_data[\"hypothesis\"] = eval_data.apply(lambda row: extract_hyps(row), axis=1)\n",
    "\n",
    "eval_batch = [[p, h] for p,h in zip(eval_data[\"premise\"].values, eval_data[\"hypothesis\"].values)]\n",
    "eval_batch[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Roberta large MNLI fine-tuned on MultiNLI\n",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/roberta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"neutral\": 1,\n",
    "             \"entailment\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "batch = collate_tokens(\n",
    "    [model.encode(pair[0], pair[1]) for pair in eval_batch], pad_idx=1\n",
    ")\n",
    "predictions = model.predict('mnli', batch).argmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "results.append(model_result)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3877182/3201005820.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mdel\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty_cache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DistilRoberta-base fine-tuned on SNLI and MultiNLI\n",
    "cross-encoder/nli-distilroberta-base\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"entailment\": 1,\n",
    "             \"neutral\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-distilroberta-base')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('cross-encoder/nli-distilroberta-base')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "results.append(model_result)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bart-large fine-tuned on MultiNLI\n",
    "facebook/bart-large-mnli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_map = {\"contradiction\": 0,\n",
    "             \"neutral\": 1,\n",
    "             \"entailment\": 2\n",
    "             }\n",
    "gold_labels = [class_map[el] for el in eval_data[\"counter_label\"]]\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "features = tokenizer(eval_batch,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.cuda()\n",
    "features = features.to('cuda')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    predictions = [score_max for score_max in scores.argmax(dim=1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_result = evaluate_classifier(predictions, gold_labels, eval_metrics)\n",
    "results.append(model_result)\n",
    "model_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
