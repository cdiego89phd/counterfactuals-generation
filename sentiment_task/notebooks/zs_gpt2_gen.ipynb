{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Describe experiments</h1>\n",
    "\n",
    "When generating, we exploit the functionalities of the OpenPrompt library to manage prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import datetime\n",
    "import itertools\n",
    "import bs4\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import transformers\n",
    "import openprompt\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms.lm import LMTokenizerWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment's params read from yaml file\n"
     ]
    }
   ],
   "source": [
    "READ_SETTINGS_FROM_FILE = True\n",
    "SETTINGS_PATH = \"/home/diego/counterfactuals-generation/sentiment_task/zs_gpt2_experiments/settings/\"\n",
    "SETTING_NAME = \"zs_prompt_1_validation.yaml\"\n",
    "\n",
    "if READ_SETTINGS_FROM_FILE:\n",
    "    a_yaml_file = open(f\"{SETTINGS_PATH}{SETTING_NAME}\")\n",
    "    parsed_yaml_file = yaml.load(a_yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "    # the following params will be included in a yaml file\n",
    "    RESULTS_PATH = parsed_yaml_file['RESULTS_PATH']\n",
    "    SST2_MODEL_PATH = parsed_yaml_file['RESULTS_PATH']\n",
    "    RANDOM_SEED_SHUFFLE = parsed_yaml_file['RANDOM_SEED_SHUFFLE']\n",
    "    AUGMENT_VALSET = parsed_yaml_file['AUGMENT_VALSET']\n",
    "    KEEP_FIRST_N = parsed_yaml_file['KEEP_FIRST_N']\n",
    "    FOLDS = parsed_yaml_file['FOLDS']\n",
    "    MODEL_NAME = parsed_yaml_file['MODEL_NAME']\n",
    "    SPECIAL_TOKENS = parsed_yaml_file['SPECIAL_TOKENS']\n",
    "    TEMPLATE_PROMPT = parsed_yaml_file['TEMPLATE_PROMPT']\n",
    "    MAP_LABELS = parsed_yaml_file['MAP_LABELS']\n",
    "    ON_CUDA = parsed_yaml_file['ON_CUDA']\n",
    "    PARALLELIZATION = parsed_yaml_file['PARALLELIZATION']\n",
    "    GEN_ARGS = parsed_yaml_file['GEN_ARGS']\n",
    "    print(\"Experiment's params read from yaml file\")\n",
    "else:\n",
    "\n",
    "    # the following params will be included in a yaml file\n",
    "    RESULTS_PATH = \"/home/diego/counterfactuals-generation/sentiment_task/zs_gpt2_experiments/results/\"\n",
    "    SST2_MODEL_PATH = \"/home/diego/counterfactuals-generation/\"\n",
    "\n",
    "    RANDOM_SEED_SHUFFLE = 22\n",
    "    AUGMENT_VALSET = True\n",
    "    KEEP_FIRST_N = 1\n",
    "\n",
    "    FOLDS = [\"0\"] # FOLDS = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    MODEL_NAME = 'gpt2' #{gpt2 (gpt2-small, 12 layers), gpt2-medium (24 layers), gpt2-large (36 layers), gpt2-xl (48 layers),\n",
    "                        # gpt2-fine-tuned-sst2 }\n",
    "    # SPECIAL_TOKENS = {\"bos_token\": \"<|BOS|>\",\n",
    "    #                   \"eos_token\": \"<|EOS|>\",\n",
    "    #                   \"unk_token\": \"<|UNK|>\",\n",
    "    #                   # \"pad_token\": \"<|PAD|>\",\n",
    "    #                   \"pad_token\": \"<|EOS|>\",\n",
    "    #                   \"sep_token\": \"<|SEP|>\"} # or set it to None\n",
    "    SPECIAL_TOKENS = {\"pad_token\": \"<|endoftext|>\"} # or set it to None\n",
    "    # SPECIAL_TOKENS = None\n",
    "\n",
    "    # set the template for prompting\n",
    "    # TEMPLATE_PROMPT = \"<bos_token><label_ex> review:<sep><example_text><sep><label_counter> review:<sep>\"\n",
    "    TEMPLATE_PROMPT = \"<label_ex> review:<example_text> <label_counter> review:\"\n",
    "    MAP_LABELS = {0:\"Negative\", 1:\"Positive\"}\n",
    "\n",
    "    # CFGs on generation\n",
    "    ON_CUDA = False\n",
    "    PARALLELIZATION = False\n",
    "    GEN_ARGS = {\n",
    "        \"no_repeat_ngram_size\": [4, 8, 12],\n",
    "        \"num_beams\": [5, 10],\n",
    "        \"repetition_penalty\": [1.0, 2.0],\n",
    "        \"temperature\": [0.7]\n",
    "    }\n",
    "    print(\"Experiment's params read from notebook\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Load language model objects and example of generation</h3>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tokenizer!\n",
      "Len of tokenizer before adding tokens:50257\n",
      "Added special tokens to tokenizer!\n",
      "Len of tokenizer after adding tokens:50257\n",
      "Downloaded tokenizer, model and cfg!\n"
     ]
    }
   ],
   "source": [
    "# Load language model objects\n",
    "if MODEL_NAME == \"gpt2-fine-tuned-sst2\":\n",
    "    load_path = f\"{SST2_MODEL_PATH}{MODEL_NAME}\"\n",
    "else: # load model from the hugging face repository\n",
    "    load_path = MODEL_NAME\n",
    "\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(load_path)\n",
    "print(\"Downloaded tokenizer!\")\n",
    "if SPECIAL_TOKENS is not None:\n",
    "    print(f\"Len of tokenizer before adding tokens:{len(tokenizer)}\")\n",
    "    tokenizer.add_special_tokens(SPECIAL_TOKENS) # add special tokens\n",
    "    print(\"Added special tokens to tokenizer!\")\n",
    "    print(f\"Len of tokenizer after adding tokens:{len(tokenizer)}\")\n",
    "\n",
    "# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
    "lm_config_class = transformers.GPT2Config.from_pretrained(load_path, pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "lm = transformers.GPT2LMHeadModel.from_pretrained(load_path, config=lm_config_class)\n",
    "if SPECIAL_TOKENS is not None:\n",
    "    #Special tokens added, model needs to be resized accordingly\n",
    "    lm.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# load lm class for the tokenizer (for the generation with openprompt)\n",
    "tokenizer_wrapper = LMTokenizerWrapper\n",
    "\n",
    "print(\"Downloaded tokenizer, model and cfg!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2Config {\n  \"_name_or_path\": \"/home/diego/counterfactuals-generation/gpt2-fine-tuned-sst2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"pad_token_id\": 50256,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.14.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_config_class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='/home/diego/counterfactuals-generation/gpt2-fine-tuned-sst2', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello, I am \\xa0a very good friend of yours.\\nI have been a fan since the beginning and my first book was published in 1999 by HarperCollins. It is one that has inspired me to write more books about myself than any other author ever did before (and it\\'s also an inspiration for many others). The story behind this novel follows two young women who are both on their way back from college when they meet up with someone named \"The Man\" at his house after he leaves'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, I am \"\n",
    "generated = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "lm.eval()\n",
    "\n",
    "out = lm.generate(generated, max_length=100,\n",
    "                  temperature=0.9,\n",
    "                  repetition_penalty=2.0)\n",
    "tokenizer.decode(out[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Clarification about the generation</h3>\n",
    "When generating counterfactuals in a few-shot learning scenario, we DO NOT add special tokens to the tokenizer/model, because the pre-trained model was not pre-trained to recognise such tokens. Differently, when fine-tuning the model, we can force the model to learn this new special tokens to guide (more granurarly) the counterfactual generation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def load_raw_dataset(loading_path):\n",
    "    train = pd.read_csv(loading_path + \"training_set\", sep='\\t')\n",
    "    val = pd.read_csv(loading_path + \"val_set\", sep='\\t')\n",
    "    test = pd.read_csv(loading_path + \"test_set\", sep='\\t')\n",
    "    return train, val, test\n",
    "\n",
    "def augment_dataset(df_dataset):\n",
    "    booked_ids = df_dataset[\"paired_id\"].values\n",
    "    examples = df_dataset[\"example\"].values\n",
    "    labels_ex = df_dataset[\"label_ex\"].values\n",
    "    counters = df_dataset[\"counterfactual\"].values\n",
    "    labels_counters = df_dataset[\"label_counter\"].values\n",
    "\n",
    "    ids = generate_custom_ids(booked_ids)\n",
    "    d = {\"paired_id\": ids,\n",
    "         \"example\": counters,\n",
    "         \"label_ex\": labels_counters,\n",
    "         \"counterfactual\": examples,\n",
    "         \"label_counter\": labels_ex}\n",
    "    new_df = pd.DataFrame(data=d)\n",
    "\n",
    "    # append the new df\n",
    "    df_final = pd.concat([df_dataset, new_df], ignore_index=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def generate_custom_ids(idxs):\n",
    "    max_id = max(idxs)\n",
    "    return [i for i in range(max_id + 1, max_id + 1 + len(idxs))]\n",
    "\n",
    "def wrap_with_prompt(df_row, template):\n",
    "    final_text = template.replace(\"<label_ex>\", MAP_LABELS[df_row[\"label_ex\"]])\n",
    "    final_text = final_text.replace(\"<example_text>\", df_row[\"example\"])\n",
    "    final_text = final_text.replace(\"<label_counter>\", MAP_LABELS[df_row[\"label_counter\"]])\n",
    "\n",
    "    if SPECIAL_TOKENS is not None and \"sep_token\" in SPECIAL_TOKENS:\n",
    "        final_text = final_text.replace(\"<sep>\", SPECIAL_TOKENS[\"sep_token\"])\n",
    "    if SPECIAL_TOKENS is not None and \"bos_token\" in SPECIAL_TOKENS:\n",
    "        final_text = final_text.replace(\"<bos_token>\", SPECIAL_TOKENS[\"bos_token\"])\n",
    "    return final_text\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, raw_dataframe):\n",
    "        # get a copy of the dataframe\n",
    "        self.raw_dataframe = raw_dataframe.copy(deep=True)\n",
    "        self.guids = []\n",
    "        self.dataset = {}\n",
    "\n",
    "    # convert the Dataframe into the InputExample format dataset of openprompt\n",
    "    def prepare_dataloader(self):\n",
    "        for index, row in self.raw_dataframe.iterrows():\n",
    "            self.dataset[row['paired_id']] = InputExample(guid=row['paired_id'],\n",
    "                                                          text_a=bs4.BeautifulSoup(\n",
    "                                                              row['wrapped_input'], \"lxml\").text,\n",
    "                                                          meta={\"label_ex\":row['label_ex'],\n",
    "                                                                \"label_counter\":row['label_counter'],\n",
    "                                                                'example':bs4.BeautifulSoup(\n",
    "                                                                    row['example'], \"lxml\").text,\n",
    "                                                                'counterfactual':bs4.BeautifulSoup(\n",
    "                                                                    row['counterfactual'], \"lxml\").text})\n",
    "            self.guids.append(row['paired_id'])\n",
    "        print('Dataloader prepared!')\n",
    "\n",
    "    # the same of __getitem__\n",
    "    def get_instance_by_id(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    # implemented because of inheritance from Dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # implemented because of inheritance from Dataset\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    def __next__(self):\n",
    "        return iter(self.dataset)\n",
    "\n",
    "    # implemented because of inheritance from Dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.__getitem__(idx)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "    def get_raw_dataframe(self):\n",
    "        return self.raw_dataframe\n",
    "\n",
    "def set_generator(template, plm, parallelization, cuda_gen):\n",
    "\n",
    "    prompt = openprompt.PromptForGeneration(\n",
    "        template = template,\n",
    "        freeze_plm = True,\n",
    "        plm = plm,\n",
    "        plm_eval_mode = True\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available() and cuda_gen:\n",
    "        prompt = prompt.cuda()\n",
    "\n",
    "    if parallelization:\n",
    "        prompt.parallelize()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "class CounterGenerator:\n",
    "    def __init__(self, dataloader, dataset, generator, tok, cfgs):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataset\n",
    "        self.generator = generator\n",
    "        self.tok = tok\n",
    "        self.gen_cfgs = cfgs\n",
    "\n",
    "    def perform_generation(self, on_cuda):\n",
    "        self.generator.eval()\n",
    "        if torch.cuda.is_available() and on_cuda:\n",
    "            print(f\"Total GPU memory available: {torch.cuda.get_device_properties(0).total_memory}\")\n",
    "            print(f\"Allocated GPU memory before generation: {torch.cuda.memory_allocated(0)}\")\n",
    "            print(f\"Allocated GPU memory reserved: {torch.cuda.memory_reserved(0)}\")\n",
    "\n",
    "        for (step, inputs) in enumerate(self.dataloader):\n",
    "\n",
    "            # retrieve the instance involved\n",
    "            instance_guid = inputs[\"guid\"].numpy()[0]\n",
    "            instance_to_update = self.dataset.get_instance_by_id(instance_guid)\n",
    "\n",
    "            # we limit the output length to be reasonably equal to the input\n",
    "            # context, i.e. the example\n",
    "            max_length_example = len(self.tok.encode(instance_to_update.text_a))\n",
    "            max_length_output = int(2 * max_length_example)\n",
    "\n",
    "            # cfg_gen[0] = no_repeat_ngram_size\n",
    "            # cfg_gen[1] = num_beam\n",
    "            # cfg_gen[2] = temperature\n",
    "            generation_arguments = {\n",
    "                \"max_length\": max_length_output,\n",
    "                \"min_length\": 5,\n",
    "                \"no_repeat_ngram_size\": self.gen_cfgs[0],\n",
    "                \"num_beams\": self.gen_cfgs[1],\n",
    "                \"repetition_penalty\": self.gen_cfgs[2],\n",
    "                \"temperature\": self.gen_cfgs[3],\n",
    "                \"do_sample\": False,\n",
    "                \"top_k\": 10,\n",
    "                \"top_p\": 0,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                if torch.cuda.is_available() and on_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                _, generated_counter = self.generator.generate(inputs,\n",
    "                                                               verbose=False,\n",
    "                                                               **generation_arguments)\n",
    "\n",
    "                # insert the generated counterfactual\n",
    "                instance_to_update.meta[\"generated_counter\"] = generated_counter[0]\n",
    "                print(generated_counter)\n",
    "\n",
    "            except Exception as e:\n",
    "                instance_to_update.meta[\"generated_counter\"] = None\n",
    "                print(instance_guid)\n",
    "                print(e)\n",
    "\n",
    "            if (step % 100) == 0 and (step > 0):\n",
    "                print(f\"{datetime.datetime.now()}, Step:{step}: 100 counterfactuals generated\")\n",
    "\n",
    "    def dataframe_from_dataset(self):\n",
    "        paired_ids = [idx for idx in self.dataset]\n",
    "        labels_ex = [self.dataset.get_instance_by_id(idx).meta[\"label_ex\"] for idx in self.dataset]\n",
    "        examples = [self.dataset.get_instance_by_id(idx).meta[\"example\"] for idx in self.dataset]\n",
    "        labels_counter = [self.dataset.get_instance_by_id(idx).meta[\"label_counter\"] for idx in self.dataset]\n",
    "        counterfactuals = [self.dataset.get_instance_by_id(idx).meta[\"counterfactual\"] for idx in self.dataset]\n",
    "        generated_counters = [self.dataset.get_instance_by_id(idx).meta[\"generated_counter\"] for idx in self.dataset]\n",
    "        d = {\"paired_id\":paired_ids,\n",
    "             \"label_ex\":labels_ex,\n",
    "             \"example\":examples,\n",
    "             \"label_counter\":labels_counter,\n",
    "             \"counterfactual\":counterfactuals,\n",
    "             \"generated_counter\":generated_counters\n",
    "             }\n",
    "        return pd.DataFrame(data=d)\n",
    "\n",
    "    def print_generation(self, path_to_print, args):\n",
    "        # create a dataframe from dataset\n",
    "        df_to_print = self.dataframe_from_dataset()\n",
    "\n",
    "        # print such dataframe\n",
    "        filename = f\"{path_to_print[:-5]}-{args}.gen\"\n",
    "        df_to_print.to_csv(filename, sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Generate counterfactuals</h3>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-10 17:29:47.940444: Beginning generation for fold 0\n",
      "\n",
      "Generation parameters: (4, 5, 1.0, 0.7)\n",
      "# of instances in the validation set:390\n",
      "Augmented dataset - # of instances in the validation set:780\n",
      "Dataloader prepared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 342.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is one of the best movies I have ever seen. It is very well written, well paced, and very well acted. The story is very well told, the characters are very well developed, and the action is very well choreographed. I would recommend this movie to anyone who is looking for a good action movie. It is a very well written and well acted movie, and I would recommend it to anyone who has never seen a movie like this before. I would also recommend this movie if you are looking for an action movie that is well paced, well choreographed, and well acted. It is one of my all time favorite movies of all time, and I highly recommend it to everyone who has ever watched this movie. I highly recommend this movie for anyone looking for a great action movie, and for anyone who is a fan of action movies. It is the best action movie of all time. I would highly recommend it for anyone who has ever seen this movie, and would recommend it for everyone who has never watched any other action movie. I']\n",
      "2022-03-10 17:30:08.097907: Printing generation...\n",
      "2022-03-10 17:30:08.099580: Finished to print...\n",
      "\n",
      "Generation parameters: (4, 5, 2.0, 0.7)\n",
      "# of instances in the validation set:390\n",
      "Augmented dataset - # of instances in the validation set:780\n",
      "Dataloader prepared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 417.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_426757/1960243037.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     59\u001B[0m                                              \u001B[0mgen_args\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m                                              )\n\u001B[0;32m---> 61\u001B[0;31m         \u001B[0mcounter_generator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperform_generation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mON_CUDA\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0;31m# print the generated counterfactuals\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_426757/3396221597.py\u001B[0m in \u001B[0;36mperform_generation\u001B[0;34m(self, on_cuda)\u001B[0m\n\u001B[1;32m    149\u001B[0m                 _, generated_counter = self.generator.generate(inputs,\n\u001B[1;32m    150\u001B[0m                                                                \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m                                                                **generation_arguments)\n\u001B[0m\u001B[1;32m    152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m                 \u001B[0;31m# insert the generated counterfactual\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, batch, verbose, **generation_kwargs)\u001B[0m\n\u001B[1;32m    488\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate_ith_token\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m                 \u001B[0moutput_sequence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minstance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minput_generation_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpad_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_token_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meos_token_id\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meos_token_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m                 \u001B[0moutput_sequences\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_sequence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# TODO: to support generate multiple sentence\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1078\u001B[0m                 \u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1079\u001B[0m                 \u001B[0msynced_gpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msynced_gpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1080\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1081\u001B[0m             )\n\u001B[1;32m   1082\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mbeam_search\u001B[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1809\u001B[0m                 \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1810\u001B[0m                 \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1811\u001B[0;31m                 \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1812\u001B[0m             )\n\u001B[1;32m   1813\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/openprompt/pipeline_base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m         \"\"\"\n\u001B[1;32m    416\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_generation_function\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1057\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1058\u001B[0m         )\n\u001B[1;32m   1059\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    893\u001B[0m                     \u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    894\u001B[0m                     \u001B[0muse_cache\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_cache\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 895\u001B[0;31m                     \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    896\u001B[0m                 )\n\u001B[1;32m    897\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    399\u001B[0m             \u001B[0mhead_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhead_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    400\u001B[0m             \u001B[0muse_cache\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_cache\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 401\u001B[0;31m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    402\u001B[0m         )\n\u001B[1;32m    403\u001B[0m         \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattn_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m  \u001B[0;31m# output_attn: a, present, (attentions)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/deeptransformers/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    323\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlayer_past\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mpast_key\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpast_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer_past\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 325\u001B[0;31m             \u001B[0mkey\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpast_key\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    326\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpast_value\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "template_prompt = '{\"placeholder\":\"text_a\"}{\"mask\"}'\n",
    "prompt_template = ManualTemplate(text = template_prompt, tokenizer = tokenizer)\n",
    "\n",
    "all_pars = sorted(GEN_ARGS)\n",
    "gen_grid = list(itertools.product(*(GEN_ARGS[par] for par in all_pars)))\n",
    "\n",
    "for fold in FOLDS:\n",
    "\n",
    "    # create dir to store the results\n",
    "    res_path = f\"{RESULTS_PATH}fold_{fold}\"\n",
    "    if not os.path.exists(res_path):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(res_path)\n",
    "\n",
    "    print(f\"{datetime.datetime.now()}: Beginning generation for fold {fold}\")\n",
    "    for gen_args in gen_grid:\n",
    "        print(f\"\\nGeneration parameters: {gen_args}\")\n",
    "        # load the datasets\n",
    "        df_trainset, df_valset, df_testset = load_raw_dataset(f\"cad_imdb/fold_{fold}/\")\n",
    "\n",
    "        # shuffle valset\n",
    "        df_valset = df_valset.sample(frac=1, random_state=RANDOM_SEED_SHUFFLE)\n",
    "\n",
    "        # whether to duplicate the data by inverting example-counter the intances\n",
    "        print(f\"# of instances in the validation set:{len(df_valset)}\")\n",
    "        if AUGMENT_VALSET:\n",
    "            df_valset = augment_dataset(df_valset)\n",
    "            print(f\"Augmented dataset - # of instances in the validation set:{len(df_valset)}\")\n",
    "\n",
    "        # whether to reduce the valset\n",
    "        if KEEP_FIRST_N > 0:\n",
    "            df_valset = df_valset.head(KEEP_FIRST_N)\n",
    "\n",
    "        # wrap the datasets with the prompt template\n",
    "        df_valset[\"wrapped_input\"] = df_valset.apply(lambda row: wrap_with_prompt(row, TEMPLATE_PROMPT), axis=1)\n",
    "\n",
    "        # prepare the data loader\n",
    "        valset = SentimentDataset(raw_dataframe=df_valset)\n",
    "        valset.prepare_dataloader()\n",
    "\n",
    "        val_data_loader = openprompt.PromptDataLoader(\n",
    "            dataset = list(valset.get_dataset().values()),\n",
    "            tokenizer = tokenizer,\n",
    "            template = prompt_template,\n",
    "            tokenizer_wrapper_class=tokenizer_wrapper\n",
    "        )\n",
    "\n",
    "        # set the prompt for generation\n",
    "        prompt_for_generation = set_generator(prompt_template,\n",
    "                                              lm,\n",
    "                                              PARALLELIZATION,\n",
    "                                              ON_CUDA)\n",
    "\n",
    "        # generate counterfactuals\n",
    "        counter_generator = CounterGenerator(val_data_loader,\n",
    "                                             valset,\n",
    "                                             prompt_for_generation,\n",
    "                                             tokenizer,\n",
    "                                             gen_args\n",
    "                                             )\n",
    "        counter_generator.perform_generation(ON_CUDA)\n",
    "\n",
    "        # print the generated counterfactuals\n",
    "        print(f\"{datetime.datetime.now()}: Printing generation...\")\n",
    "        counter_generator.print_generation(f\"{RESULTS_PATH}fold_{fold}/{SETTING_NAME}\", gen_args)\n",
    "        print(f\"{datetime.datetime.now()}: Finished to print...\")\n",
    "\n",
    "    print(f\"{datetime.datetime.now()}: Generation completed for fold {fold}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_426757/1542608772.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# df_valset.head(2)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvalset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_426757/1542608772.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# df_valset.head(2)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvalset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/snap/dataspell/4/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/dataspell/4/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/dataspell/4/plugins/python-ce/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/snap/dataspell/4/plugins/python-ce/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# trainset.get_dataset()\n",
    "# train_data_loader.raw_dataset\n",
    "# df_valset.head(2)\n",
    "for v in valset.get_dataset():\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "   paired_id                                            example  label_ex  \\\n0          4  Long, boring, blasphemous. Never have I been s...         0   \n1         13  If you haven't seen this, it's incredible. It ...         1   \n\n                                      counterfactual  label_counter  \n0  Long, fascinating, soulful. Never have I been ...              1  \n1  If you haven't seen this, it's terrible. It is...              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paired_id</th>\n      <th>example</th>\n      <th>label_ex</th>\n      <th>counterfactual</th>\n      <th>label_counter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>Long, boring, blasphemous. Never have I been s...</td>\n      <td>0</td>\n      <td>Long, fascinating, soulful. Never have I been ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13</td>\n      <td>If you haven't seen this, it's incredible. It ...</td>\n      <td>1</td>\n      <td>If you haven't seen this, it's terrible. It is...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trainset.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "# text = valset.get_dataset()[18436].text_a\n",
    "# generated = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "# # lm.eval()\n",
    "#\n",
    "# out = lm.generate(generated, max_length=tokenizer.model_max_length)\n",
    "# tokenizer.decode(out[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# print()\n",
    "# text = \"Hello, I am \"\n",
    "# generated = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "# # lm.eval()\n",
    "#\n",
    "# out = lm.generate(generated, max_length=tokenizer.model_max_length)\n",
    "# tokenizer.decode(out[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[15496,    11,   314,   716,   220,  1849,    64,   845,   922,  1545,\n           286, 12431,    13,   198,    40,   423,   587,   257,  4336,  1201,\n           262,  3726,   290,   616,   717,  1492,   373,  3199,   287,  7358,\n           416, 12686, 49645,   764,   632,   318,   530,   326,   468,  7867,\n           502,   284,  3551,   517,  3835,   546,  3589,   621,   597,   584,\n          1772,  1683,   750,   878,   357,   392,   340,   338,   635,   281,\n         12141,   329,   867,  1854,   737,   383,  1621,  2157,   428,  5337,\n          5679,   734,  1862,  1466,   508,   389,  1111,   319,   511,   835,\n           736,   422,  4152,   618,   484,  1826,   510,   351,  2130,  3706,\n           366,   464,  1869,     1,   379,   465,  2156,   706,   339,  5667]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}