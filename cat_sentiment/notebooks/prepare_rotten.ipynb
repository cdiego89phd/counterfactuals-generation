{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Rotten movies</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.89k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ba8ad9eb3e64477a6f5c15583e3def6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/921 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f3edc0a31bf4ceb8366660237dc8cf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset rotten_tomatoes_movie_review/default (download: 476.34 KiB, generated: 1.28 MiB, post-processed: Unknown size, total: 1.75 MiB) to /home/diego/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/488k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "becced7e33da4164bd8795d7f29054d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "400f235b566f46e8a5237c745224de05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31085266c5b94bd3a79b807afc69f662"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c668d3ef3274da584c185f5c0607d4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rotten_tomatoes_movie_review downloaded and prepared to /home/diego/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46f2a645800d4246a15fbc8a4f796fd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rotten_data = load_dataset(\"rotten_tomatoes\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Total length of polarity rotten test {len(rotten_data['test'])}\")\n",
    "print(f\"Total length of polarity rotten train {len(rotten_data['train'])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we keep train for training and test for testing as it is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of polarity yelp test 1066\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                text  label sentiment\n0  lovingly photographed in the manner of a golde...      1  positive\n1              consistently clever and suspenseful .      1  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lovingly photographed in the manner of a golde...</td>\n      <td>1</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>consistently clever and suspenseful .</td>\n      <td>1</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we merge train and test\n",
    "\n",
    "texts = rotten_data[\"test\"][\"text\"]\n",
    "labels = rotten_data[\"test\"][\"label\"]\n",
    "\n",
    "d = {\"text\": texts,\n",
    "     \"label\": labels,\n",
    "}\n",
    "\n",
    "df_rotten = pd.DataFrame(data=d)\n",
    "df_rotten[\"sentiment\"] = df_rotten.apply(lambda row: \"positive\" if row['label'] else \"negative\", axis=1)\n",
    "df_rotten.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "8530"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rotten_data['train'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts = df_yelp[\"text\"].values\n",
    "lens = [len(el.split(\" \")) for el in texts]\n",
    "print(f\"Max len: {np.max(lens)}\")\n",
    "print(f\"Mean len: {np.mean(lens)}\")\n",
    "print(f\"Min len: {np.min(lens)}\")\n",
    "\n",
    "df_yelp[\"sentiment\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filter out reviews longer than 2000 and shorter than 10 words\n",
    "texts = df_yelp[\"text\"].values\n",
    "df_yelp[\"review_len\"] = [len(el.split(\" \")) for el in texts]\n",
    "df_yelp = df_yelp[(df_yelp[\"review_len\"] <= 2000) & (df_yelp[\"review_len\"] >= 10)].copy()\n",
    "\n",
    "lens = [len(el) for el in df_yelp[\"text\"].values]\n",
    "print(f\"Max len: {np.max(lens)}\")\n",
    "print(f\"Mean len: {np.mean(lens)}\")\n",
    "print(f\"Min len: {np.min(lens)}\")\n",
    "\n",
    "df_yelp[\"label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_yelp[\"label_counter\"] = [int(not el) for el in df_yelp[\"label\"]]\n",
    "df_yelp[\"sentiment_counter\"] = [\"positive\" if x==\"negative\" else \"negative\" for x in df_yelp[\"sentiment\"]]\n",
    "df_yelp.rename(columns={\"label\": \"label_ex\",\n",
    "                        \"text\": \"example\",\n",
    "                        \"sentiment\": \"sentiment_ex\"}, inplace=True)\n",
    "df_yelp.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pos = df_yelp[df_yelp[\"label_ex\"] == 1].copy()\n",
    "df_neg = df_yelp[df_yelp[\"label_ex\"] == 0].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample 10k instances - 5k positive, 5k negative. Train-test split 80%-20%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "# sample positive and negative labels\n",
    "data_pos = df_pos.sample(n=5000, replace=False, random_state=seed)\n",
    "data_neg = df_neg.sample(n=5000, replace=False, random_state=seed)\n",
    "\n",
    "# sample train-test split positive labels (80-20)%\n",
    "test_pos = data_pos.sample(frac=0.2, replace=False, random_state=seed)\n",
    "train_pos = data_pos[~data_pos.index.isin(test_pos.index)]\n",
    "\n",
    "# sample train-test split negative labels (80-20)%\n",
    "test_neg = data_neg.sample(frac=0.2, replace=False, random_state=seed)\n",
    "train_neg = data_neg[~data_neg.index.isin(test_neg.index)]\n",
    "\n",
    "# build train and test\n",
    "testset = test_neg.append(test_pos)\n",
    "trainset = train_neg.append(train_pos)\n",
    "\n",
    "print()\n",
    "print(f\"len test: {len(testset)}\")\n",
    "print(f\"len train: {len(trainset)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testset.to_csv(\"../yelp/test.csv\", sep=\"\\t\", index=False)\n",
    "trainset.to_csv(\"../yelp/train.csv\", sep=\"\\t\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def produce_datasets(n, m, out_dir):\n",
    "     train_set = pd.read_csv(\"../yelp/train.csv\", sep=\"\\t\")\n",
    "     print(len(train_set))\n",
    "\n",
    "     df_origin = train_set.sample(n=n+m, replace=False, random_state=seed)\n",
    "     df_n = df_origin.sample(n=n, replace=False, random_state=seed)\n",
    "     df_seed = df_n.sample(n=m, replace=False, random_state=seed)\n",
    "\n",
    "     print(f\"len origin: {len(df_origin)}\")\n",
    "     print(f\"len n_data: {len(df_n)}\")\n",
    "     print(f\"len seed: {len(df_seed)}\")\n",
    "\n",
    "     df_origin = trainset.sample(n=n+m, replace=False, random_state=seed)\n",
    "     df_n = df_origin.sample(n=n, replace=False, random_state=seed)\n",
    "     df_seed = df_n.sample(n=m, replace=False, random_state=seed)\n",
    "\n",
    "     print(f\"len origin: {len(df_origin)}\")\n",
    "     print(f\"len n_data: {len(df_n)}\")\n",
    "     print(f\"len seed: {len(df_seed)}\")\n",
    "\n",
    "     df_seed[\"paired_id\"] = [i for i in range(len(df_seed))]\n",
    "     df_seed[\"counterfactual\"] = [\"None\" for i in range(len(df_seed))]\n",
    "\n",
    "     df_origin.to_csv(f\"../yelp/{out_dir}/origin_data.csv\", sep=\"\\t\", index=False)\n",
    "     df_n.to_csv(f\"../yelp/{out_dir}/n_data.csv\", sep=\"\\t\", index=False)\n",
    "     df_seed.to_csv(f\"../yelp/{out_dir}/seed_data.csv\", sep=\"\\t\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## n=5k & m=2.5k (n=2m)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "produce_datasets(5000, 2500, \"tr2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "produce_datasets(2000, 1000, \"tr1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final size of each training set will be m+m+m=2m+m=n+m. The following sets are sampled from train.csv\n",
    "\n",
    "- origin.csv stores n+m original data points\n",
    "- m_data.csv\n",
    "- seed_data.csv is a sample (size m) from n_data\n",
    "\n",
    "You now need to produce m countefactuals from seed_data!! You will then use n_data.csv and m generated counterfactuals to train your classfier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
