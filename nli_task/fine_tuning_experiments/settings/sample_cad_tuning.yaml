DATASET_PATH: "~/counterfactuals-generation/nli_task/cad_flickr_nli/"
OUT_DIR: "~/counterfactuals-generation/nli_task/fine_tuning_experiments/saved_models"

FOLD: "0"
IS_SWEEP: False

MODEL_FROM_LOCAL: False
BASE_MODEL: "sshleifer/tiny-gpt2"
LM_NAME: "hivemind/gpt-j-6B-8bit" #{gpt2 (gpt2-small, 12 layers), gpt2-medium (24 layers), gpt2-large (36 layers), gpt2-xl (48 layers)}

TO_FREEZE_LAYERS: False
UNFREEZE_LAST_N: 6 # The last N layers to unfreeze for training
SPECIAL_TOKENS: # or set it to None
  "bos_token": "<|BOS|>"
  "eos_token": "<|EOS|>"
  "unk_token": "<|UNK|>"
  "pad_token": "<|PAD|>"
  "sep_token": "<|SEP|>"
TOKENIZE_IN_BATCH: False
NO_CUDA: False

PROMPT_ID: "1"
TEMPLATE_PROMPT: "<bos_token>Label: <original_label>\n<sep>P: <P>\n<sep>H: <H>\n<sep>Counterfactual Label: <counter_label>\n<sep><TC>: <counter_text><eos_token>"

TRAINING_CFGS:
  MAX_EPOCHS: 30
  TRAIN_BATCHSIZE: 4
  EVAL_BATCHSIZE: 4
  BATCH_UPDATE: 4
  WARMUP_STEPS: 0
  LR: 0.00025
  ADAM_EPS: 0.000000001
  WEIGHT_DECAY: 0.0
  STOPPING_PATIENCE: 3
  FREEZE_LAYERS: False
  UNFREEZE_LAST_N: 6
