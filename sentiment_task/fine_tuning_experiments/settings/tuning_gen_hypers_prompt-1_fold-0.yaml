DATASET_PATH: "/home/diego/counterfactuals-generation/sentiment_task/cad_imdb/"
OUT_DIR: "/home/diego/counterfactuals-generation/sentiment_task/fine_tuning_experiments/validation/trained_models"
MODEL_DIR: "/home/diego/counterfactuals-generation/sentiment_task/fine_tuning_experiments/saved_models"
FOLD: "0" # ["0", "1", "2", "3", "4"]
N_SWEEP_RUNS: 1


LM_NAME: "sshleifer/tiny-gpt2" #{gpt2 (gpt2-small, 12 layers), gpt2-medium (24 layers), gpt2-large (36 layers), gpt2-xl (48 layers)}
TO_FREEZE_LAYERS: False
UNFREEZE_LAST_N: 6 # The last N layers to unfreeze for training
SPECIAL_TOKENS: # or set it to None
  "bos_token": "<|BOS|>"
  "eos_token": "<|EOS|>"
  "unk_token": "<|UNK|>"
  "pad_token": "<|PAD|>"
  "sep_token": "<|SEP|>"
TOKENIZE_IN_BATCH: True
CUDA_DEVICE: 0  # set to -1 for cpu

CLASSIFIER_NAME: "distilbert-base-uncased-finetuned-sst-2-english"

TEMPLATE_PROMPT: "<bos_token><label_ex> review:<sep><example_text><sep><label_counter> review:<sep><counter_text><eos_token>"
GENERATION_PROMPT: "<bos_token><label_ex> review:<sep><example_text><sep><label_counter> review:<sep>"
MAP_LABELS:
  0: "Negative"
  1: "Positive"

TRAIN_CFG:
  EPOCHS: 1
  EPS: 0.1
  EVAL_BATCHSIZE: 4
  LR: 0.1
  TRAIN_BATCHSIZE: 4
  BATCH_UPDATE: 4
  WARMUP_STEPS: 1
  WEIGHT_DECAY: 0.1

